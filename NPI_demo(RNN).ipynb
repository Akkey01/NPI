{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48546e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import NPI\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58f09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from pathlib import Path; import csv\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def per_roi_std(signals):  # signals: [T,N]\n",
    "    return signals.std(axis=0, ddof=1) + 1e-8\n",
    "\n",
    "def make_snr_vector_uneven(N, center, sigma=0.4, seed=42):\n",
    "    \"\"\"\n",
    "    ROI-wise SNR around a center (5 or 10). Larger sigma = more heterogeneity.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    factors = rng.lognormal(mean=0.0, sigma=sigma, size=N)\n",
    "    factors = np.clip(factors, 0.3, 3.0)          # keep sane\n",
    "    return center * factors\n",
    "\n",
    "def add_noise_by_snr(signals, snr_per_roi):\n",
    "    T, N = signals.shape\n",
    "    sig_std = per_roi_std(signals)\n",
    "    noise_std = sig_std / snr_per_roi\n",
    "    return signals + np.random.randn(T, N) * noise_std\n",
    "\n",
    "def pearson_r(a, b):\n",
    "    a = a.reshape(-1); b = b.reshape(-1)\n",
    "    ma, mb = a.mean(), b.mean()\n",
    "    num = ((a-ma)*(b-mb)).sum()\n",
    "    den = np.sqrt(((a-ma)**2).sum() * ((b-mb)**2).sum()) + 1e-12\n",
    "    return float(num/den)\n",
    "\n",
    "def log_row(path, row):\n",
    "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_header = not path.exists()\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if write_header: w.writeheader()\n",
    "        w.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0837c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_LSTM(nn.Module):\n",
    "    def __init__(self, node_dim, hidden=128, num_layers=2, steps=3):\n",
    "        super().__init__()\n",
    "        self.node_dim, self.steps = node_dim, steps\n",
    "        self.enc = nn.Linear(node_dim, hidden)\n",
    "        self.lstm = nn.LSTM(input_size=hidden, hidden_size=hidden,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden, node_dim)\n",
    "        self.to(device)\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:               # allow (steps*N,)\n",
    "            x = x.unsqueeze(0)         # -> (1, steps*N)\n",
    "        B = x.shape[0]\n",
    "        seq = x.view(B, self.steps, self.node_dim)\n",
    "        h = torch.relu(self.enc(seq))\n",
    "        h, _ = self.lstm(h)\n",
    "        out = self.out(h[:, -1, :])    # (B, N)\n",
    "        return out[0] if out.shape[0] == 1 else out   # <-- return (N,) for unbatched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4a4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "    def forward(self, x):  # x: (B,T,D)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class ANN_Transformer(nn.Module):\n",
    "    def __init__(self, node_dim, d_model=128, nhead=8, num_layers=3, steps=3):\n",
    "        super().__init__()\n",
    "        self.node_dim, self.steps = node_dim, steps\n",
    "        self.inp = nn.Linear(node_dim, d_model)\n",
    "        self.pos = PositionalEncoding(d_model, max_len=steps)\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
    "        self.out = nn.Linear(d_model, node_dim)\n",
    "        self.to(device)\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:               # allow (steps*N,)\n",
    "            x = x.unsqueeze(0)         # -> (1, steps*N)\n",
    "        B = x.shape[0]\n",
    "        seq = x.view(B, self.steps, self.node_dim)   # (B, T, N)\n",
    "        h = self.inp(seq)\n",
    "        h = self.pos(h)\n",
    "        h = self.encoder(h)\n",
    "        out = self.out(h[:, -1, :])    # (B, N)\n",
    "        return out[0] if out.shape[0] == 1 else out   # <-- return (N,) for unbatched               # next-step prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e94db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN_masked(model, input_X, target_Y, steps, p_node=0.15,\n",
    "                    batch_size=64, train_set_proportion=0.8,\n",
    "                    num_epochs=80, lr=5e-4, l2=5e-5):\n",
    "    Xtr = torch.tensor(input_X[:int(train_set_proportion*len(input_X))], dtype=torch.float, device=device)\n",
    "    ytr = torch.tensor(target_Y[:int(train_set_proportion*len(target_Y))], dtype=torch.float, device=device)\n",
    "    Xte = torch.tensor(input_X[int(train_set_proportion*len(input_X)):], dtype=torch.float, device=device)\n",
    "    yte = torch.tensor(target_Y[int(train_set_proportion*len(target_Y)):], dtype=torch.float, device=device)\n",
    "    tr = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Xtr, ytr), batch_size, shuffle=True)\n",
    "    te = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Xte, yte), batch_size, shuffle=False)\n",
    "\n",
    "    loss = nn.MSELoss(); opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
    "    train_hist, test_hist = [], []\n",
    "    N = ytr.shape[1]\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in tr:\n",
    "            # reshape, mask random ROIs across ~60% of timesteps\n",
    "            B = xb.size(0)\n",
    "            xv = xb.view(B, steps, N)\n",
    "            mask = torch.zeros_like(xv, dtype=torch.bool)\n",
    "            num_nodes = max(1, int(p_node*N))\n",
    "            t_keep = max(1, int(0.4*steps))\n",
    "            t_idx = torch.randint(0, steps, (B, t_keep), device=device)\n",
    "            for b in range(B):\n",
    "                n_idx = torch.randperm(N, device=device)[:num_nodes]\n",
    "                mask[b, :, n_idx] = True\n",
    "                mask[b, t_idx[b], :] = False  # keep some timesteps unmasked\n",
    "            xc = xv.masked_fill(mask, 0.0).view(B, steps*N)\n",
    "\n",
    "            pred = model(xc)\n",
    "            l = loss(pred, yb)\n",
    "            opt.zero_grad(); l.backward(); opt.step()\n",
    "\n",
    "        # epoch eval\n",
    "        model.eval(); tl=0; tn=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in tr:\n",
    "                tl += loss(model(xb), yb).item()*yb.size(0); tn += yb.size(0)\n",
    "            train_hist.append(tl/tn); tl=0; tn=0\n",
    "            for xb, yb in te:\n",
    "                tl += loss(model(xb), yb).item()*yb.size(0); tn += yb.size(0)\n",
    "            test_hist.append(tl/tn)\n",
    "    return model, train_hist, test_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3498d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_block(model, X, y, steps, true_signals=None, real_EC=None, SC=None):\n",
    "    N = y.shape[1]\n",
    "    out = {}\n",
    "    model_fc = NPI.model_FC(model, node_num=N, steps=steps); out[\"model_fc\"]=model_fc\n",
    "    if true_signals is not None:\n",
    "        emp_fc = NPI.corrcoef(true_signals)\n",
    "        mask = ~np.eye(N, dtype=bool)\n",
    "        out[\"r_fc\"] = pearson_r(model_fc[mask], emp_fc[mask])\n",
    "    ec  = NPI.model_EC(model, X, y, pert_strength=1.0); out[\"ec\"]=ec\n",
    "    jac = NPI.model_Jacobian(model, X, steps=steps);   out[\"jac\"]=jac\n",
    "    if real_EC is not None: out[\"r_ec_real\"]=pearson_r(ec, real_EC); out[\"r_jac_real\"]=pearson_r(jac, real_EC)\n",
    "    if SC is not None:      out[\"r_ec_sc\"]=pearson_r(ec, SC)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3809b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- baseline run config (no noise notebook) ---\n",
    "SEED = 42\n",
    "\n",
    "import random, numpy as np\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    torch = None\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch:\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15c9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_without_diagonal(matrix):\n",
    "\n",
    "    \"Flatten the matrix without including the diagonal\"\n",
    "    \n",
    "    n = matrix.shape[0]\n",
    "    flattened = []\n",
    "    for i in range(n):\n",
    "        for j in list(range(i)) + list(range(i + 1, n)):\n",
    "            flattened.append(matrix[i][j])\n",
    "    \n",
    "    return np.array(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fd7d2",
   "metadata": {},
   "source": [
    "### **NPI usage demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f82aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size              = 50\n",
    "train_set_proportion    = 0.8\n",
    "ROI_num                 = 20\n",
    "using_steps             = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b472cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 1, 42]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# load the matching world for this seed (recommended)\n",
    "signals_clean = np.loadtxt(f'./RNN_simulation_data/sim_data/dynamics_020nodes_8000steps_{seed:02d}seed.txt')\n",
    "SC      = np.loadtxt(f'./RNN_simulation_data/sim_data/SC_020nodes_8000steps_{seed:02d}seed.txt')\n",
    "real_EC = np.loadtxt(f'./RNN_simulation_data/sim_data/real_EC_020nodes_8000steps_{seed:02d}seed.txt')\n",
    "\n",
    "signals = signals_clean.copy()      # clean reference\n",
    "N = signals_clean.shape[1]\n",
    "steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3a2cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "N, steps = 20, 3\n",
    "dummy = torch.randn(steps*N, device=device)  # 1-D\n",
    "print(ANN_LSTM(N, steps=steps)(dummy).shape)         # torch.Size([1, N])\n",
    "print(ANN_Transformer(N, steps=steps)(dummy).shape)  # torch.Size([1, N])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Path(\"metrics\").mkdir(parents=True, exist_ok=True)\n",
    "results_csv = \"metrics/uneven_snr_5_10_mlp_lstm_mask.csv\"\n",
    "\n",
    "for center in [5, 10]:  # medium & high SNR\n",
    "    snr_vec = make_snr_vector_uneven(N, center=center, sigma=0.4, seed=seed)  # uneven SNR\n",
    "    noisy = add_noise_by_snr(signals_clean, snr_vec)\n",
    "    X, y = NPI.multi2one(noisy, steps)\n",
    "\n",
    "    # ----- MLP (baseline) -----\n",
    "    mlp = NPI.ANN_MLP(input_dim=steps*N, hidden_dim=int(2.5*N), latent_dim=int(0.8*N), output_dim=N)\n",
    "    mlp, tr, te = NPI.train_NN(mlp, X, y, num_epochs=80, lr=1e-3, l2=5e-5)\n",
    "    out = eval_block(mlp, X, y, steps, true_signals=signals_clean, real_EC=real_EC if 'real_EC' in globals() else None,\n",
    "                     SC=SC if 'SC' in globals() else None)\n",
    "    log_row(results_csv, {\"seed\": seed,\"model\":\"MLP\",\"mask\":\"no\",\"SNR_center\":center,\"train\":tr[-1],\"test\":te[-1],\n",
    "                          \"r_fc\":out.get(\"r_fc\",\"\"),\"r_ec_real\":out.get(\"r_ec_real\",\"\"),\n",
    "                          \"r_jac_real\":out.get(\"r_jac_real\",\"\"),\"r_ec_sc\":out.get(\"r_ec_sc\",\"\")})\n",
    "\n",
    "    # ----- MLP + masking -----\n",
    "    mlp_m = NPI.ANN_MLP(input_dim=steps*N, hidden_dim=int(2.5*N), latent_dim=int(0.8*N), output_dim=N)\n",
    "    mlp_m, tr, te = train_NN_masked(mlp_m, X, y, steps, p_node=0.15, num_epochs=80, lr=1e-3, l2=5e-5)\n",
    "    out = eval_block(mlp_m, X, y, steps, true_signals=signals_clean, real_EC=real_EC if 'real_EC' in globals() else None,\n",
    "                     SC=SC if 'SC' in globals() else None)\n",
    "    log_row(results_csv, {\"seed\": seed,\"model\":\"MLP\",\"mask\":\"yes\",\"SNR_center\":center,\"train\":tr[-1],\"test\":te[-1],\n",
    "                          \"r_fc\":out.get(\"r_fc\",\"\"),\"r_ec_real\":out.get(\"r_ec_real\",\"\"),\n",
    "                          \"r_jac_real\":out.get(\"r_jac_real\",\"\"),\"r_ec_sc\":out.get(\"r_ec_sc\",\"\")})\n",
    "\n",
    "    # ----- LSTM (baseline) -----\n",
    "    lstm = ANN_LSTM(node_dim=N, hidden=128, num_layers=2, steps=steps)\n",
    "    lstm, tr, te = NPI.train_NN(lstm, X, y, num_epochs=80, lr=5e-4, l2=5e-5)\n",
    "    out = eval_block(lstm, X, y, steps, true_signals=signals_clean, real_EC=real_EC if 'real_EC' in globals() else None,\n",
    "                     SC=SC if 'SC' in globals() else None)\n",
    "    log_row(results_csv, {\"seed\": seed,\"model\":\"LSTM\",\"mask\":\"no\",\"SNR_center\":center,\"train\":tr[-1],\"test\":te[-1],\n",
    "                          \"r_fc\":out.get(\"r_fc\",\"\"),\"r_ec_real\":out.get(\"r_ec_real\",\"\"),\n",
    "                          \"r_jac_real\":out.get(\"r_jac_real\",\"\"),\"r_ec_sc\":out.get(\"r_ec_sc\",\"\")})\n",
    "\n",
    "    # ----- LSTM + masking -----\n",
    "    lstm_m = ANN_LSTM(node_dim=N, hidden=128, num_layers=2, steps=steps)\n",
    "    lstm_m, tr, te = train_NN_masked(lstm_m, X, y, steps, p_node=0.15, num_epochs=80, lr=5e-4, l2=5e-5)\n",
    "    out = eval_block(lstm_m, X, y, steps, true_signals=signals_clean, real_EC=real_EC if 'real_EC' in globals() else None,\n",
    "                     SC=SC if 'SC' in globals() else None)\n",
    "    log_row(results_csv, {\"seed\": seed,\"model\":\"LSTM\",\"mask\":\"yes\",\"SNR_center\":center,\"train\":tr[-1],\"test\":te[-1],\n",
    "                          \"r_fc\":out.get(\"r_fc\",\"\"),\"r_ec_real\":out.get(\"r_ec_real\",\"\"),\n",
    "                          \"r_jac_real\":out.get(\"r_jac_real\",\"\"),\"r_ec_sc\":out.get(\"r_ec_sc\",\"\")})\n",
    "    \n",
    "    # ----- Transformer (baseline) -----\n",
    "    tfm = ANN_Transformer(node_dim=N, d_model=128, nhead=8, num_layers=3, steps=steps)\n",
    "    tfm, tr, te = NPI.train_NN(tfm, X, y, num_epochs=80, lr=5e-4, l2=5e-5)\n",
    "    out = eval_block(tfm, X, y, steps, true_signals=signals_clean, real_EC=real_EC, SC=SC)\n",
    "    log_row(results_csv, {\"seed\": seed,\"model\":\"Transformer\",\"mask\":\"no\",\"SNR_center\":center,\"train\":tr[-1],\"test\":te[-1],\n",
    "                          \"r_fc\":out.get(\"r_fc\",\"\"),\"r_ec_real\":out.get(\"r_ec_real\",\"\"),\n",
    "                          \"r_jac_real\":out.get(\"r_jac_real\",\"\"),\"r_ec_sc\":out.get(\"r_ec_sc\",\"\")})\n",
    "\n",
    "    # ----- Transformer + masking -----\n",
    "    tfm_m = ANN_Transformer(node_dim=N, d_model=128, nhead=8, num_layers=3, steps=steps)\n",
    "    tfm_m, tr, te = train_NN_masked(tfm_m, X, y, steps, p_node=0.15, num_epochs=80, lr=5e-4, l2=5e-5)\n",
    "    out = eval_block(tfm_m, X, y, steps, true_signals=signals_clean, real_EC=real_EC, SC=SC)\n",
    "    log_row(results_csv, {\"seed\": seed,\"model\":\"Transformer\",\"mask\":\"yes\",\"SNR_center\":center,\"train\":tr[-1],\"test\":te[-1],\n",
    "                          \"r_fc\":out.get(\"r_fc\",\"\"),\"r_ec_real\":out.get(\"r_ec_real\",\"\"),\n",
    "                          \"r_jac_real\":out.get(\"r_jac_real\",\"\"),\"r_ec_sc\":out.get(\"r_ec_sc\",\"\")})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b983f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CONTROL FLAGS ======\n",
    "RUN_OLD_DEMO = False          # <- set to True if you want to see the old single-seed demo again\n",
    "RUN_GROUP_BASELINE = False    # <- set to True if you want the 50-seed baseline loop\n",
    "# ===========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6c843",
   "metadata": {},
   "source": [
    "##### train NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bc080",
   "metadata": {},
   "source": [
    "Several ANNs (MLP, CNN, RNN, VAR) are provided in the NPI framework, which can be used as a surrogate brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15803cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = NPI.multi2one(signals, steps = using_steps)\n",
    "\n",
    "ANN = NPI.ANN_MLP(input_dim = using_steps * ROI_num, hidden_dim = 2 * ROI_num, latent_dim = int(0.8 * ROI_num), output_dim = ROI_num)\n",
    "# ANN = NPI.ANN_CNN(in_channels = ROI_num, hidden_channels = 3 * ROI_num, out_channels = ROI_num, data_length = using_steps)\n",
    "# ANN = NPI.ANN_RNN(input_dim = ROI_num, hidden_dim = int(2.5 * ROI_num), latent_dim = int(2.5 * ROI_num), output_dim = ROI_num, data_length = using_steps)\n",
    "# ANN = NPI.ANN_VAR(input_dim = using_steps * ROI_num, output_dim = ROI_num)\n",
    "\n",
    "ANN, training_loss, testing_loss = NPI.train_NN(ANN, inputs, targets, batch_size, train_set_proportion, num_epochs = 80, lr = 2e-4, l2 = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d790649",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss, label = 'Training loss')\n",
    "plt.plot(testing_loss, label = 'Testing loss')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c56064c",
   "metadata": {},
   "source": [
    "##### calculate empirical FC & model FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_FC = NPI.corrcoef(signals)\n",
    "model_FC = NPI.model_FC(ANN, node_num = ROI_num, steps = using_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff3a0b",
   "metadata": {},
   "source": [
    "##### calculate NPI Inferred EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083051ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPI_EC = NPI.model_EC(ANN, inputs, targets, pert_strength = 1.0)\n",
    "np.fill_diagonal(NPI_EC, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971cdf2",
   "metadata": {},
   "source": [
    "##### calculate model Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPI_Jacobian = NPI.model_Jacobian(ANN, inputs, steps = using_steps)\n",
    "np.fill_diagonal(NPI_Jacobian, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787875dc",
   "metadata": {},
   "source": [
    "##### empirical FC - model FC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 6))\n",
    "sns.heatmap(empirical_FC, ax = ax1, vmin = -0.5, vmax = 0.5, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "sns.heatmap(model_FC, ax = ax2, vmin = -1.0, vmax = 1.0, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "ax1.set_title('Empirical FC'); ax2.set_title('Model FC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value, p_value = pearsonr(flat_without_diagonal(model_FC), flat_without_diagonal(empirical_FC))\n",
    "\n",
    "plt.figure(figsize = (4.8, 4.8))\n",
    "plt.scatter(flat_without_diagonal(model_FC), flat_without_diagonal(empirical_FC))\n",
    "\n",
    "plt.xlim(-1.05, 1.05); plt.xticks([-1.0, 0.0, 1.0]); plt.xlabel('model FC')\n",
    "plt.ylim(-0.55, 0.55); plt.yticks([-0.5, 0.0, 0.5]); plt.ylabel('empirical FC')\n",
    "plt.text(-0.95, 0.45, 'r = {:.2f}, p = {:.0e}'.format(r_value, p_value))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False); ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d824297",
   "metadata": {},
   "source": [
    "##### real EC - NPI EC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 6))\n",
    "sns.heatmap(real_EC, ax = ax1, vmin = -0.2, vmax = 0.2, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "sns.heatmap(NPI_EC, ax = ax2, vmin = -0.2, vmax = 0.2, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "ax1.set_title('Real EC'); ax2.set_title('NPI Inferred EC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value, p_value = pearsonr(flat_without_diagonal(NPI_EC), flat_without_diagonal(real_EC))\n",
    "\n",
    "plt.figure(figsize = (4.8, 4.8))\n",
    "plt.scatter(flat_without_diagonal(NPI_EC), flat_without_diagonal(real_EC))\n",
    "\n",
    "plt.xlim(-0.25, 0.25); plt.xticks([-0.2, 0.0, 0.2]); plt.xlabel('NPI Inferred EC')\n",
    "plt.ylim(-0.25, 0.25); plt.yticks([-0.2, 0.0, 0.2]); plt.ylabel('Real EC')\n",
    "plt.text(-0.2, 0.2, 'r = {:.2f}, p = {:.0e}'.format(r_value, p_value))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False); ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1f381",
   "metadata": {},
   "source": [
    "##### real EC - model Jacobian comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 6))\n",
    "sns.heatmap(real_EC, ax = ax1, vmin = -0.2, vmax = 0.2, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "sns.heatmap(NPI_Jacobian, ax = ax2, vmin = -0.2, vmax = 0.2, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "ax1.set_title('Real EC'); ax2.set_title('Model Jacobian')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d10113",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value, p_value = pearsonr(flat_without_diagonal(NPI_Jacobian), flat_without_diagonal(real_EC))\n",
    "\n",
    "plt.figure(figsize = (4.8, 4.8))\n",
    "plt.scatter(flat_without_diagonal(NPI_Jacobian), flat_without_diagonal(real_EC))\n",
    "\n",
    "plt.xlim(-0.25, 0.25); plt.xticks([-0.2, 0.0, 0.2]); plt.xlabel('Model Jacobian')\n",
    "plt.ylim(-0.25, 0.25); plt.yticks([-0.2, 0.0, 0.2]); plt.ylabel('Real EC')\n",
    "plt.text(-0.2, 0.2, 'r = {:.2f}, p = {:.0e}'.format(r_value, p_value))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False); ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d378a0",
   "metadata": {},
   "source": [
    "##### SC - NPI EC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f863ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 6))\n",
    "sns.heatmap(SC, ax = ax1, vmin = -0.7, vmax = 0.7, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "sns.heatmap(NPI_EC, ax = ax2, vmin = -0.2, vmax = 0.2, cmap = 'RdBu_r', cbar = False, square = True, xticklabels = False, yticklabels = False)\n",
    "ax1.set_title('SC'); ax2.set_title('NPI Inferred EC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf982a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_value, p_value = pearsonr(flat_without_diagonal(NPI_EC), flat_without_diagonal(SC))\n",
    "\n",
    "plt.figure(figsize = (4.8, 4.8))\n",
    "plt.scatter(flat_without_diagonal(NPI_EC), flat_without_diagonal(SC))\n",
    "\n",
    "plt.xlim(-0.25, 0.25); plt.xticks([-0.2, 0.0, 0.2]); plt.xlabel('NPI Inferred EC')\n",
    "plt.ylim(-0.75, 0.75); plt.yticks([-0.7, 0.0, 0.7]); plt.ylabel('SC')\n",
    "plt.text(-0.2, 0.6, 'r = {:.2f}, p = {:.0e}'.format(r_value, p_value))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False); ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9516ba",
   "metadata": {},
   "source": [
    "##### Group-level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPIFC_empiricalFC_corr = []\n",
    "NPIEC_realEC_corr = []\n",
    "NPIEC_SC_corr = []\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    signals = np.loadtxt('./RNN_simulation_data/sim_data/dynamics_020nodes_8000steps_{:02d}seed.txt'.format(i))\n",
    "    SC      = np.loadtxt('./RNN_simulation_data/sim_data/SC_020nodes_8000steps_{:02d}seed.txt'.format(i))\n",
    "    real_EC = np.loadtxt('./RNN_simulation_data/sim_data/real_EC_020nodes_8000steps_{:02d}seed.txt'.format(i))\n",
    "\n",
    "    inputs, targets = NPI.multi2one(signals, steps = 3)\n",
    "    ANN = NPI.ANN_MLP(input_dim = using_steps * ROI_num, hidden_dim = 2 * ROI_num, latent_dim = int(0.8 * ROI_num), output_dim = ROI_num)\n",
    "    ANN, training_loss, testing_loss = NPI.train_NN(ANN, inputs, targets, num_epochs = 80, lr = 2e-4, l2 = 5e-5)\n",
    "\n",
    "    empirical_FC = NPI.corrcoef(signals)\n",
    "    model_FC = NPI.model_FC(ANN, node_num = ROI_num, steps = 3)\n",
    "    NPIFC_empiricalFC_corr.append(pearsonr(flat_without_diagonal(model_FC), flat_without_diagonal(empirical_FC))[0])\n",
    "\n",
    "    NPI_EC = NPI.model_EC(ANN, inputs, targets, pert_strength = 1.0)\n",
    "    np.fill_diagonal(NPI_EC, 0)\n",
    "    NPIEC_realEC_corr.append(pearsonr(flat_without_diagonal(NPI_EC), flat_without_diagonal(real_EC))[0])\n",
    "    NPIEC_SC_corr.append(pearsonr(flat_without_diagonal(NPI_EC), flat_without_diagonal(SC))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['NPI FC -\\n real FC', 'NPI EC -\\n real EC', 'NPI EC -\\n SC']\n",
    "values = [np.mean(NPIFC_empiricalFC_corr), np.mean(NPIEC_realEC_corr), np.mean(NPIEC_SC_corr)]\n",
    "stds = [np.std(NPIFC_empiricalFC_corr), np.std(NPIEC_realEC_corr), np.std(NPIEC_SC_corr)]\n",
    "\n",
    "plt.figure(figsize = (3, 3))\n",
    "plt.bar(categories, values, yerr = stds, capsize = 5)\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False); ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0b25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "def pick(*names):\n",
    "    g = globals()\n",
    "    for n in names:\n",
    "        if n in g:\n",
    "            return g[n]\n",
    "    raise NameError(f\"Could not find any of: {names}\")\n",
    "\n",
    "def upper_tri(m):\n",
    "    m = np.asarray(m)\n",
    "    i = np.triu_indices_from(m, k=1)\n",
    "    return m[i]\n",
    "\n",
    "def fc_corr_from_any(emp_fc, mdl_fc):\n",
    "    a = upper_tri(emp_fc)\n",
    "    b = upper_tri(mdl_fc)\n",
    "    return float(np.corrcoef(a, b)[0, 1])\n",
    "\n",
    "# Try common names used in FC comparison cells\n",
    "emp_fc = pick(\"empirical_FC\", \"empirical_fc\", \"FC_empirical\", \"FC_emp\")\n",
    "mdl_fc = pick(\"model_FC\", \"model_fc\", \"FC_model\", \"fc_model\")\n",
    "\n",
    "fc_corr = fc_corr_from_any(emp_fc, mdl_fc)\n",
    "\n",
    "row = {\n",
    "    \"demo\": \"RNN\",          # change to \"WBM\" if you're in that notebook\n",
    "    \"snr\": \"baseline\",\n",
    "    \"mode\": \"none\",\n",
    "    \"seed\": SEED,\n",
    "    \"metric\": \"fc_corr\",\n",
    "    \"value\": fc_corr,\n",
    "}\n",
    "csv_path = \"results_snr_sweep.csv\"\n",
    "pd.DataFrame([row]).to_csv(csv_path, mode=\"a\", header=not os.path.exists(csv_path), index=False)\n",
    "print(\"Appended to\", csv_path, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6dbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plots for inference from uneven-SNR experiments ===\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(\"metrics/uneven_snr_5_10_mlp_lstm_mask.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Canonical order for models/masks\n",
    "order = [(\"MLP\",\"no\"),(\"MLP\",\"yes\"),\n",
    "         (\"LSTM\",\"no\"),(\"LSTM\",\"yes\"),\n",
    "         (\"Transformer\",\"no\"),(\"Transformer\",\"yes\")]\n",
    "\n",
    "metrics = [\"r_fc\",\"r_ec_real\",\"r_jac_real\",\"r_ec_sc\"]\n",
    "\n",
    "# Save dir\n",
    "plot_dir = Path(\"metrics/graphs\"); plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for snr in sorted(df[\"SNR_center\"].unique()):\n",
    "    sub = df[df[\"SNR_center\"]==snr]\n",
    "    for metric in metrics:\n",
    "        if metric not in sub.columns: continue\n",
    "        plt.figure(figsize=(8,4))\n",
    "        means = []\n",
    "        stds  = []\n",
    "        labels = []\n",
    "        for model,mask in order:\n",
    "            vals = sub[(sub[\"model\"]==model)&(sub[\"mask\"]==mask)][metric].dropna()\n",
    "            if len(vals)==0: continue\n",
    "            means.append(vals.mean()); stds.append(vals.std(ddof=1)); labels.append(f\"{model}|{mask}\")\n",
    "        x = range(len(means))\n",
    "        plt.bar(x, means, yerr=stds, capsize=4)\n",
    "        plt.xticks(x, labels, rotation=30, ha=\"right\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{metric} @ SNR {snr} (mean ± std over seeds)\")\n",
    "        plt.tight_layout()\n",
    "        outpath = plot_dir / f\"{metric}_SNR{snr}.png\"\n",
    "        plt.savefig(outpath, dpi=200)\n",
    "        plt.show()\n",
    "        print(\"Saved:\", outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2717f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ All experiments complete. Plots saved in metrics/graphs/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
